# -*- coding: utf-8 -*-
"""Sompalli-ML-project2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aCAgeETWJxdwpsTNV6_fzS210uDCJrOT
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
import warnings
from sklearn.exceptions import ConvergenceWarning

# Ignore warnings for clean output
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=ConvergenceWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# Load the dataset
data = pd.read_csv('nba.csv')

# Inspect the dataset
print(data.head())  # View first few rows
print(data.info())  # Check data types and missing values
print(data['TAR'].value_counts())  # Check target class distribution (class imbalance check)

# Drop the 'Name' column as it is not useful for classification
data = data.drop(columns=['Name'])

# Check for missing values
missing_values = data.isnull().sum()
print(missing_values)

# Drop rows with missing values to ensure model integrity
# Alternative: We could use imputation, but dropping is simpler if missing values are minimal
data = data.dropna()

# Feature Engineering: Create a new feature "Efficiency"
# This metric considers offensive and defensive contributions while penalizing inefficiency
# Avoid division by zero using 1e-9

data['Efficiency'] = (data['FGM'] + data['REB'] + data['AST'] + data['STL'] + data['BLK']) / (data['FGA'] + data['TOV'] + 1e-9)

# Normalize numerical features using StandardScaler
# StandardScaler ensures features have zero mean and unit variance, improving model performance
scaler = StandardScaler()
numerical_features = data.drop(columns=['TAR']).columns
data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Train-Test Split (80% training, 20% testing)
X = data.drop(columns=['TAR'])
y = data['TAR']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Data Visualization: Target variable distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='TAR', data=data, palette='Set2')
plt.title('Target Variable Distribution (TAR)')
plt.xlabel('Target (0: <5 years, 1: >=5 years)')
plt.ylabel('Count')
plt.show()

# Correlation Matrix
plt.figure(figsize=(12, 8))
corr_matrix = data.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Feature Correlation Matrix')
plt.show()

# K-Nearest Neighbors Classifier with GridSearchCV
knn = KNeighborsClassifier()
param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}
grid_search = GridSearchCV(knn, param_grid, cv=10, scoring='f1')
grid_search.fit(X_train, y_train)
y_pred_knn = grid_search.predict(X_test)
print("Best KNN Parameters:", grid_search.best_params_)
print("KNN F1 Score:", f1_score(y_test, y_pred_knn))

# Random Forest Classifier with GridSearchCV
rf = RandomForestClassifier(random_state=42)
param_grid = {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]}
grid_search = GridSearchCV(rf, param_grid, cv=10, scoring='f1')
grid_search.fit(X_train, y_train)
y_pred_rf = grid_search.predict(X_test)
print("Best RF Parameters:", grid_search.best_params_)
print("RF F1 Score:", f1_score(y_test, y_pred_rf))

# Logistic Regression with GridSearchCV
log_reg = LogisticRegression(solver='saga', max_iter=1000, random_state=42)
param_grid = {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10]}
grid_search = GridSearchCV(log_reg, param_grid, cv=10, scoring='f1')
grid_search.fit(X_train, y_train)
y_pred_lr = grid_search.predict(X_test)
print("Best Logistic Regression Parameters:", grid_search.best_params_)
print("Logistic Regression F1 Score:", f1_score(y_test, y_pred_lr))

# Artificial Neural Network (MLP Classifier) with GridSearchCV
ann = MLPClassifier(random_state=42)
param_grid = {'hidden_layer_sizes': [(50,), (100,)], 'activation': ['relu', 'tanh']}
grid_search = GridSearchCV(ann, param_grid, cv=10, scoring='f1')
grid_search.fit(X_train, y_train)
y_pred_ann = grid_search.predict(X_test)
print("Best ANN Parameters:", grid_search.best_params_)
print("ANN F1 Score:", f1_score(y_test, y_pred_ann))

# Compare Models Based on F1 Score
f1_scores = {
    "KNN": f1_score(y_test, y_pred_knn),
    "Random Forest": f1_score(y_test, y_pred_rf),
    "Logistic Regression": f1_score(y_test, y_pred_lr),
    "Artificial Neural Network": f1_score(y_test, y_pred_ann)
}
# Select Best Model
best_model_name = max(f1_scores, key=f1_scores.get)
best_f1_score = f1_scores[best_model_name]
print(f"Best Model: {best_model_name} with F1 Score: {best_f1_score}")

# Ensure the best model's predictions are used for evaluation
y_pred_best = {
    "KNN": y_pred_knn,
    "Random Forest": y_pred_rf,
    "Logistic Regression": y_pred_lr,
    "Artificial Neural Network": y_pred_ann
}[best_model_name]

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_best)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.title("Confusion Matrix")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_best)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()